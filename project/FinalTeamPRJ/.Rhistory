(0.31776  * mtcars$vs)   +
( 2.52023 * mtcars$am)   +
(0.65541  * mtcars$gear) +
(-0.19942 * mtcars$carb)
summary(model.mt)
summary(model.mt)
summary(model.mt)
mydata <- read.csv( "https://stats.idre.ucla.edu/stat/data/binary.csv" )
str(mydata)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는
#로지스틱 모델을 만드시오(0: 불합격, 1:합격).
str(my.data)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는
#로지스틱 모델을 만드시오(0: 불합격, 1:합격).
str(my.data)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는
#로지스틱 모델을 만드시오(0: 불합격, 1:합격).
str(mydata)
mydata <- read.csv( "https://stats.idre.ucla.edu/stat/data/binary.csv" )
str(mydata)
#(1) gre, gpa, rank를 이용해 합격 여부(admit)를 예측하는
#로지스틱 모델을 만드시오(0: 불합격, 1:합격).
str(mydata)
mydata <- iris[,1:4]
fit <- kmeans( x= mydata,center =3)
fit
fit$cluster
fit$cluster
fit$centers
library(cluster)
clusplot(mydata,fit$cluster,
colot = T,
shade = T,
lbels =2,
lines =1)
library(cluster)
clusplot(mydata,fit$cluster,
colot = T,
shade = T,
lbels =2,
lines =1)
watnings()
warnings()
clusplot(mydata,fit$cluster,
color = T,
shade = T,
lbels =2,
lines =1)
summary(model.mt)
warnings()
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =2,
lines =1)
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =2,
lines =0)
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =1,
lines =0)   #중심점 선 - 1은 선 有 0은 선 무
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =0,  #label 0은   label 1은 숫자대신 모양
lines =0)   #중심점 선 - 1은 선 有 0은 선 無
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =2,  #label 0은   label 1은 숫자대신 모양
lines =0)   #중심점 선 - 1은 선 有 0은 선 無
library(cluster)
clusplot(mydata,fit$cluster,
color = T,
shade = T,
labels =2,  #label 0은   label 1은 숫자대신 모양
lines =0)   #중심점 선 - 1은 선 有 0은 선 無
clusplot(mydata, #군집대상
fit$cluster,  #군집번호
color = T,    #원의색
shade = T,    #원의 빗금표시유무
labels =2,  #관측값 출력형태_label 2는 숫자 , label 1은 모양
lines =1)   #중심점 연결 표시여부_ 1은 선 有,0은 선 無
fit <- kmeans( x= mydata,center =3) ;fit
subset(mydata, fit$cluster == 2)
std <- function(x){
return(( x - min ( x ))/(max(x) - min(x)))
}
mydata <- apply( iris[ , 1:4 ],2, std)
fit <- kmean(x = mydata , center =3)
fir
fit
View(iris[ , 1:4 ])
mydata <- apply( iris[ , 1:4 ],2, std)
View(mydata)
library(class)
#훈련옹 /테스트용 데이터 준비
tr.idx <- c(1:25,51:75,101:125)
ds.tr <- iris[tr.idx,1:4]; ds.tr #훈련용
#훈련옹 /테스트용 데이터 준비
tr.idx <- c(1:25,51:75,101:125) ;tr.idx
ds.tr <- iris[tr.idx,1:4]; ds.tr #훈련용
cl.tr <- factor(iris[tr.idx,5]) #훈련용 그룹정보
cl.tr <- factor(iris[tr.idx,5]); cl.tr #훈련용 그룹정보
cl.ts <- factor(iris[-tr.idx,5]); cl.ts#테스트 그룹정보
pred <- knn(ds.tr,ds.ts,cl.tr, k = 3, prob =TRUE)
ds.ts <- iris[-tr.idx,1:4]#테스트용
cl.ts <- factor(iris[-tr.idx,5]); cl.ts#테스트 그룹정보
pred <- knn(ds.tr,ds.ts,cl.tr, k = 3, prob =TRUE)
pred
acc <- mean(pred == cl.ts)
acc
table(pred,cl.ts)
acc
table(pred,cl.ts)
pred <- knn(ds.tr,ds.ts,cl.tr, k = 5, prob =TRUE) # k :근접한 이웃 몇개 체크할지_3개 _임의로 설정
pred                #훈련하는 함수                #prob :많은 데이터가 있는 쪽으로 따라가라 (TRUE)
acc <- mean(pred == cl.ts) #True =1 false =0 값들의 평균
acc  #예측력
table(pred,cl.ts)
pred <- knn(ds.tr,ds.ts,cl.tr, k = 1, prob =TRUE) # k :근접한 이웃 몇개 체크할지_3개 _임의로 설정
pred                #훈련하는 함수                #prob :많은 데이터가 있는 쪽으로 따라가라 (TRUE)
acc <- mean(pred == cl.ts) #True =1 false =0 값들의 평균
acc  #예측력
table(pred,cl.ts)
pred <- knn(ds.tr,ds.ts,cl.tr, k = 2, prob =TRUE) # k :근접한 이웃 몇개 체크할지_3개 _임의로 설정
pred                #훈련하는 함수                #prob :많은 데이터가 있는 쪽으로 따라가라 (TRUE)
acc <- mean(pred == cl.ts) #True =1 false =0 값들의 평균
acc  #예측력
pred <- knn(ds.tr,ds.ts,cl.tr, k = 10, prob =TRUE) # k :근접한 이웃 몇개 체크할지_3개 _임의로 설정
pred                #훈련하는 함수                #prob :많은 데이터가 있는 쪽으로 따라가라 (TRUE)
acc <- mean(pred == cl.ts) #True =1 false =0 값들의 평균
acc  #예측력
pred <- knn(ds.tr,ds.ts,cl.tr, k = 7, prob =TRUE) # k :근접한 이웃 몇개 체크할지_3개 _임의로 설정
pred                #훈련하는 함수                #prob :많은 데이터가 있는 쪽으로 따라가라 (TRUE)
acc <- mean(pred == cl.ts) #True =1 false =0 값들의 평균
acc  #예측력
table(pred,cl.ts)
install.package('cvTools')
install.packages('cvTools')
pred <- knn(ds.tr,ds.ts,cl.tr,k = 5)
acc[i] <- mean( pred == cl.ts )
mean(acc)
library(cvTools)
k = 10
folds <-  cvFolds(nrow( iris ), K =k )
acc <- c()
for( i in 1:k) {
ts.idx <-  fold$which == i
ds.tr <- iris[-ts.idx, 1:4]
ds.ts <- iris[ts.idx,1:4]
cl.tr <- factor(iris[-ts.idx,5])
cl.ts <- factor(iris[ts.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k = 5)
acc[i] <- mean( pred == cl.ts )
}
acc
mean(acc)
library(cvTools)
k = 10
folds <-  cvFolds(nrow( iris ), K =k )
acc <- c()
for( i in 1:k) {
ts.idx <-  fold$which == i
ds.tr <- iris[-ts.idx, 1:4]
ds.ts <- iris[ts.idx,1:4]
cl.tr <- factor(iris[-ts.idx,5])
cl.ts <- factor(iris[ts.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k = 5)
acc[i] <- mean( pred == cl.ts )
}
acc
mean(acc)
mean(acc)
ts.idx <-  folds$which == i
ds.tr <- iris[-ts.idx, 1:4]
ds.ts <- iris[ts.idx,1:4]
cl.tr <- factor(iris[-ts.idx,5])
cl.ts <- factor(iris[ts.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k = 5)
acc[i] <- mean( pred == cl.ts )
for( i in 1:k) {
ts.idx <-  folds$which == i
ds.tr <- iris[-ts.idx, 1:4]
ds.ts <- iris[ts.idx,1:4]
cl.tr <- factor(iris[-ts.idx,5])
cl.ts <- factor(iris[ts.idx,5])
pred <- knn(ds.tr,ds.ts,cl.tr,k = 5)
acc[i] <- mean( pred == cl.ts )
}
acc
mean(acc)
acc   #폴드별 예측정확도
mean(acc)
head(state,x77)
head(state.x77)
standard <- function(x){
return((x - min(x))/max(x)-min(x)))
}
standard <- function(x){
return((x - min(x))/max(x)-min(x)))
}
standard <- function(x){
return((x - min(x))/max(x)-min(x)))
}
standard <- function(x){
return((x - min(x))/max(x)-min(x)))
}
standard <- function(x)
standard <- function(x){}
standard <- function(x){
return((x - min(x))/max(x)-min(x)))
}
return((x - min( x ))/max(x)-min(x))
standard <- function(x){
return((x - min( x ))/max(x)-min(x))
}
standard <- function(x){
return((x - min( x ))/max(x)-min(x))
}
#• 군집의 수는 5로 한다.
#• state.x77은 각 변수(열)의 값들의 단위의 차이가 많이 나기 때문에 0~1 표준화를 실시한 후 군집화를 실행한다.
head(state.x77)
st <- apply(state.x77,2,standard)
st2)
st
st <- apply(state.x77,2,standard)
st
fit <- kmeans(x = st,centers = 5)
fit
fit <- kmeans(x = st,center = 5)
fit
library( mlbench )
data( "Sonar" ) 			# 데이터셋 불러오기
dim(sonar)
data( "Sonar" ) 			# 데이터셋 불러오기
dim(sonar)
dim(Sonar)
sn <- sonar[,-61]
sn <- Sonar[,-61]
head(sn)
Sonar
head(sn)
Sonar
group_sn <- kmeans( x= sn , center = 2)
group_sn
library(cluster)
clusplot(group_sn,
group_sn$cluster,
color = T,
shade = T,
labels= 2,
lines = 1)
group_sn
clusplot(sn,
group_sn$cluster,
color = T,
shade = T,
labels= 2,
lines = 1)
#. Sonar 데이터셋에서 마지막에 있는 Class 열이 그룹 정보이다.
#. Sonar 데이터셋에서 홀수 번째 데이터(관측값)를 훈련용 데이터로 하고, 짝수번째 데이터(관측값)를 테스트용 데이터로 한다.
#. k-최근접 이웃에서 k를 3, 5, 7로 다르게 하여 예측 정확도를 비교한다.
library(class)
Sonar
dim(Sonar)
nrow(Sonar)
odd.n <- seq(1,nrow(Sonar),2)
odd.n
doub.n <- seq(1,nrow(Sonar),1)
doub.n
doub.n <- seq(2,nrow(Sonar),2)
doub.n
sn.tr <- Sonar[odd.n,]
sn.ts <- Sonar[doub.n,]#테스트 데이터
sn.tr <- Sonar[odd.n,] ; sn.tr#훈련용 데이터
sn.tr <- Sonar[odd.n,] ; sn.tr#훈련용 데이터
sn.ts <- Sonar[doub.n,] ; sn.ts#테스트 데이터
iris[tr.idx,5]
cl.tr
cl.tr
dim(Sonar)
tr <- factor(Sonar[oodd.n,61])
tr <- factor(Sonar[odd.n,61])
ts <- factor(Sonar[doub.n,61]) # 테스트용 데이터 그룹정보 팩터화
pred <- knn(sn.tr,sn.ts,tr,k = 3, prob = TRUE)
pred <- knn(sn.tr,sn.ts,tr,k = 3, prob = TRUE) ; pred
pred
acc <- mean(pred == ts)
ts <- factor(Sonar[-odd.n,61]) # 테스트용 데이터 그룹정보 팩터화
pred <- knn(sn.tr,sn.ts,tr,k = 3, prob = TRUE) ; pred
acc <- mean(pred == ts)
acc
ts
acc
pred <- knn(sn.tr,sn.ts,tr,k = 5, prob = TRUE) ; pred
acc <- mean(pred == ts)
acc
pred <- knn(sn.tr,sn.ts,tr,k = 7, prob = TRUE) ; pred
acc <- mean(pred == ts)
#k = 5
#예측 정확도 0.9333333
acc
pred <- knn(sn.tr,sn.ts,tr,k = 3, prob = TRUE) ; pred
tr <- factor(Sonar[odd.n,61]) # 훈련용 데이터 그룹정보 팩터화
tr
tr
ts
str(Sonar)
library(cvTools)
install.packages('cvTools')
install.packages("cvTools")
folds <- cvFolds(nrow(Sonar), K = k)
library(cvTools)
k = 5
folds <- cvFolds(nrow(Sonar), K = k)
acc <- c()
acc[i] <- mean( pred == ts )
k = 5
folds <- cvFolds(nrow(Sonar), K = k)
acc <- c()
for( i in 1:k) {
doub.n <-  folds$which == i
sn.tr <- iris[-ts.idx, 1:4]
sn.ts <- iris[ts.idx,1:4]
tr <- factor(Sonar[-ts.idx,61])
ts <- factor(Sonar[ts.idx,61])
pred <- knn(sn.tr,sn.ts,tr,k = 5, prob = TRUE)
acc[i] <- mean( pred == ts )
}
library(cvTools)
k = 5
folds <- cvFolds(nrow(Sonar), K = k)
acc <- c()
for( i in 1:k) {
doub.n <-  folds$which == i
sn.tr <- iris[-ts.idx, 1:4]
sn.ts <- iris[ts.idx,1:4]
tr <- factor(Sonar[-ts.idx,61])
ts <- factor(Sonar[ts.idx,61])
pred <- knn(sn.tr,sn.ts,tr,k = 5, prob = TRUE)
acc[i] <- mean( pred == ts )
}
#Team PRJ
library(dplyr)
library(ggplot2)
setwd("C:/Bigdata Maestro/learnR/project/FinalTeamPRJ/인구")
setwd("C:/workR/learnR/project/FinalTeamPRJ/인구")
pop <- read.csv("시도 각 세별 이동자수 13-18.csv",header = F,as.is = T)
pop
inc.d <- inc %>% arrange(ratio) #내림차순
setwd("C:/workR/learnR/project/FinalTeamPRJ")
income <- read.csv("제주산업별 매출액16-17.csv",header = F,as.is = T)
View(income)
colnames(income) <- c('지역','산업','2016','2017')
rownames(income) <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
df <- data.frame(year,t1)
df2 <- data.frame(year,t2)
year <- 2016:2017
#========================================================================
str(income)
inc <- income %>%
mutate(ratio= income$`2016` / income$'2017')
View(inc)
inc <- income %>%
mutate(ratio= income$`2016` / income$'2017')
#========================================================================
str(income)
inc <- income %>%
mutate(ratio= income$`2016` / income$'2017')
View(inc)
inc.d <- inc %>% arrange(ratio) #내림차순
View(inc.d)
setwd("C:/workR/learnR/project/FinalTeamPRJ")
income <- read.csv("제주산업별 매출액16-17.csv",header = F,as.is = T)
View(income)
colnames(income) <- c('지역','산업','2016','2017')
rownames(income) <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
#숫자변환
income <- income[-c(1:2),]
income$`2016` <- as.numeric(income$`2016`)
income$`2017` <- as.numeric(income$`2017`)
ab <- t(income)
View(ab)
#숫자변환
income <- income[-c(1:2),]
income$`2016` <- as.numeric(income$`2016`)
income$`2017` <- as.numeric(income$`2017`)
ab <- t(income)
View(ab)
setwd("C:/workR/learnR/project/FinalTeamPRJ")
income <- read.csv("제주산업별 매출액16-17.csv",header = F,as.is = T)
View(income)
#숫자변환
income <- income[-c(1:2),]
View(income)
#========================================================================
str(income)
inc <- income %>%
mutate(ratio= income$`2016` / income$'2017')
View(inc)
setwd("C:/workR/learnR/project/FinalTeamPRJ")
income <- read.csv("제주산업별 매출액16-17.csv",header = F,as.is = T)
setwd("C:/workR/learnR/pr
setwd("C:/workR/learnR/pr
setwd("C:/workR/learnR/project/FinalTeamPRJ")
income <- read.csv("제주산업별 매출액16-17.csv",header = F,as.is = T)
View(income)
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
}
# 데이터 표준화
ds.new <- std( state.x77 )
head( ds.new )
# 군집화
fit <- kmeans( x = ds.new, centers = 5 )
fit
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
library( mlbench )
# 군집화
fit <- kmeans( x = ds.new, centers = 2 )
fit
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
dim( Sonar )
str( Sonar )
head( Sonar )
View( Sonar )
ds.new <- Sonar[ ,-61 ]   # Class 열 제거
head( ds.new )
# 군집화
fit <- kmeans( x = ds.new, centers = 2 )
fit
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
#문2)
#mlbench 패키지에서 제공하는 Sonar 데이터셋에 대해 k-평균 군집화를 실시하고 결과를 그래프로 출력하시오.
#
# • 군집의 수는 2로 한다.
# • Sonar 데이터셋에서 마지막에 있는 Class 열은 제외하고 군집화를 실행한다.
library(cluster)
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
library(cluster)
library( mlbench )
data( "Sonar" ) 			# 데이터셋 불러오기
dim( Sonar )
str( Sonar )
head( Sonar )
View( Sonar )
ds.new <- Sonar[ ,-61 ]   # Class 열 제거
head( ds.new )
# 군집화
fit <- kmeans( x = ds.new, centers = 2 )
fit
# 차원 축소 후 군집 시각화
clusplot( ds.new, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )
# 결측값 제거
ds.new <- Sonar[ complete.cases( Sonar ), ]
ds.new
# 훈련, 학습 데이터 생성
idx <- seq( 1, nrow( ds.new ), 2 )
# 훈련, 학습 데이터 생성
idx <- seq( 1, nrow( ds.new ), 2 );idx
x.train <- ds.new[ idx, -61 ]
ds.new[ idx, -61 ]
# 훈련, 학습 데이터 생성
idx <- seq( 1, nrow( ds.new ), 2 );idx
x.train <- ds.new[ idx, -61 ]
y.train <- ds.new$Class[ idx ];y.train
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
}
ds.new <- std( state.x77 )
ds.new <- std( state.x77 )
head( ds.new )
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
# 0~1 표준화 함수
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
}
# 데이터 표준화
ds.new <- std( state.x77 )
# 군집화
fit <- kmeans( x = ds.new, centers = 5 )
fit
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
}
# 데이터 표준화
ds.new <- std( state.x77 )
head( ds.new )
# 군집화
fit <- kmeans( x = ds.new, centers = 5 )
fit
std <- function( x ) {
result <- ( x - min( x ) ) / ( max( x ) - min( x ) )
return( result )
}
